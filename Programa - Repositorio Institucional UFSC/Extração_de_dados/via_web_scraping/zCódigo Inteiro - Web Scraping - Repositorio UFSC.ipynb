{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Versão \"Beta\" ainda, pois falta alterar a parte de extração de texto do pdfplumber para o PyMuPDF."
      ],
      "metadata": {
        "id": "z84_0QRwv-zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 1** - baixando e importanto bibliotecas utilizadas ao decorrer da execução do programa"
      ],
      "metadata": {
        "id": "ORKuc8VYgy5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fCtgPogg_c"
      },
      "outputs": [],
      "source": [
        "# Instalando as bibliotecas que serão utilizadas\n",
        "try:\n",
        "  !pip install requests\n",
        "  !pip install beautifulsoup4\n",
        "  !pip install pandas\n",
        "  !pip install regex\n",
        "  !pip install google-api-python-client\n",
        "  !pip install urllib3\n",
        "  !pip install pdfplumber\n",
        "except: # Caso ocorra algum erro ao instalar os pacotes/bibliotecas seremos notificados com uma mensagem na tela\n",
        "  print('\\nErro ao instalar pacotes/bibliotecas.')\n",
        "else:\n",
        "  from googleapiclient.discovery import build\n",
        "  from google.colab import auth\n",
        "  from google.colab import drive\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import pandas as pd\n",
        "  import time\n",
        "  import re\n",
        "  from urllib import request\n",
        "  import pdfplumber\n",
        "  import os\n",
        "  import sys\n",
        "  execucao = True # Variável que armazenará o sucesso no download das bibliotecas\n",
        "  print('='*100)\n",
        "  print('Bibliotecas carregadas!\\nPode executar o código abaixo.')\n",
        "  print('='*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 2** - Conectando ambiente do Google Colab ao Google Drive"
      ],
      "metadata": {
        "id": "lVMkG24Jg2pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  print('\\nErro ao \"sincronizar\" Google Drive com este ambiente do Google Colab.\\nExecute a célula novamente ou tente \"montar\" o Drive manualmente no botão de \"Montar Drive\" no menu lateral esquerdo.')\n",
        "  print('Descrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "  execucao = False\n",
        "else:\n",
        "  print('='*100)\n",
        "  print('Drive conectado com sucesso.\\nPode prosseguir na execução das próximas células.')\n",
        "  print('='*100)"
      ],
      "metadata": {
        "id": "zrd_Ttsgg7x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 3** - Definindo variáveis e funções que serão utilizadas ao decorrer do programa"
      ],
      "metadata": {
        "id": "nIF4bh29g9Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "  headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
        "                (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"}\n",
        "  timeout = 30\n",
        "  try:\n",
        "    def CriaDiretorio(caminho : str):\n",
        "      os.makedirs(caminho)\n",
        "      time.sleep(1)\n",
        "      if os.path.isdir(caminho):\n",
        "        print(f'Pasta {caminho} criada com sucesso.')\n",
        "        return True\n",
        "      else:\n",
        "        print(f'Tentando criar pasta {caminho} novamente.')\n",
        "        time.sleep(3)\n",
        "        if os.path.isdir(caminho):\n",
        "          print(f'Pasta {caminho} criada com sucesso.')\n",
        "          return True\n",
        "        else:\n",
        "          print(f'Pasta {caminho} não foi carregada corretamente.')\n",
        "          execucao = False\n",
        "          return False\n",
        "  except:\n",
        "    print('Erro ao criar função de criação e verificação de diretório (pasta de trabalho).\\nTente executar esta célula novamente.')\n",
        "    print('Descrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "  else:\n",
        "    print('='*100)\n",
        "    print('Variáveis para requisições e função de criação de diretório criadas com sucesso.\\nPode dar continuidade na execução das células abaixo.')\n",
        "    print('='*100)\n",
        "else:\n",
        "  print('\\nFalha no carregamento das bibliotecas. Tente executar a primeira célula de códigos novamente.\\n')"
      ],
      "metadata": {
        "id": "ZcbnyRUkg_0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 4** - Criação de pasta \"Coleções Repositorio UFSC\" e sub-pasta \"Links dos trabalhos - Repositorio UFSC\" para armazenamento dos primeiros dados (links das coleções)"
      ],
      "metadata": {
        "id": "q1LD9yH9hBmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "  try:\n",
        "    if(CriaDiretorio(\"/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC\")):\n",
        "      pass\n",
        "    else:\n",
        "      execucao = False\n",
        "      print('\\nErro na criação da pasta \"/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC\".\\nTente novamente.\\n')\n",
        "  except:\n",
        "    print('\\nErro ao sincronizar e criar pasta no Drive com o código do Google Colab.\\nTente executar a célula de códigos novamente.\\n')\n",
        "    print('Descrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "    execucao = False\n",
        "  else:\n",
        "    print('='*100)\n",
        "    print('Pastas de trabalhos \"Coleções Repositorio UFSC\" e \"Links dos trabalhos - Repositorio UFSC\" criadas com sucesso!\\nPode continuar e executar os códigos abaixo.')\n",
        "    print('='*100)\n",
        "else:\n",
        "  print('\\nFalha no carregamento das bibliotecas. Tente executar a primeira célula de códigos novamente.\\n')"
      ],
      "metadata": {
        "id": "uoxWPV7xhEUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 5** - Preenchimento de dados (links) das coleções e dos trabalhos em cada coleção"
      ],
      "metadata": {
        "id": "P3RscsgihGx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "  # Variável start armazenará o momento que o programa começar para visualizarmos, ao final,\n",
        "  # quanto tempo levou para ser executado por completo\n",
        "  start = time.time()\n",
        "\n",
        "  # Criando dicionário que armazenará possíveis avisos e/ou erros\n",
        "  lista_de_falhas = {'Avisos':[],'Erros':[]}\n",
        "\n",
        "  # Primeiro, entraremos no link da comunidade de Dissertações e Teses, para tal armazenaremos o link na variável link\n",
        "  link = 'https://repositorio.ufsc.br/handle/123456789/74645'\n",
        "\n",
        "  # Passaremos um headers com \"User-Agent\" para identificarmos para o servidor o tipo de navegador\n",
        "  # ou dispositivo que está fazendo a solicitação (boas práticas para não pensar que estamos com \"más intenções\")\n",
        "\n",
        "  try:\n",
        "    # Executando uma requisção para o link e armazenando o resultado na variável \"site\"\n",
        "    site = requests.get(link,headers=headers,timeout=timeout)\n",
        "  except:\n",
        "    print(f'Ocorreu um erro durante a requisição com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "    lista_de_falhas['Erros'].append(f'Ocorreu um erro durante a requisição com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "  else:\n",
        "    # Obtendo a resposta do status da requisição (se for igual a 200 quer dizer que foi bem sucedida)\n",
        "    res_code = site.status_code\n",
        "\n",
        "    if res_code == 200: # Se o resultado do status code for 200, prosseguiremos com a execução\n",
        "      numero_total_de_trabalhos = 0 # Criando uma variável que armazenará a quantidade de trabalhos captados\n",
        "      dic = {'Colecoes':[],'Links colecoes':[]} # Criando dicionário que armazenará as informações de nome e link das coleções\n",
        "      lista_links_trabalhos = []\n",
        "\n",
        "      # Passando o conteúdo da requisição para um formato html da BeautifulSoup\n",
        "      soup = BeautifulSoup(site.content, 'html.parser')\n",
        "\n",
        "      # Encontrando o campo que contém as informações das coleções presentes na página\n",
        "      campo_colecoes = soup.find('div', class_='ds-static-div secondary')\n",
        "      if campo_colecoes != None: # Se a busca pelo campo for bem sucedida, prosseguimos\n",
        "        colecoes = campo_colecoes.find_all('li') # Buscando todos os elementos (coleções) dentro do campo de coleções\n",
        "        if len(colecoes) == 0: # Caso o identificador dos elementos tenha mudado, ocorrerá um erro e seremos notificados\n",
        "          print('\\nNão foi possível encontrar os elementos das coleções corretamente.')\n",
        "          lista_de_falhas['Erros'].append('Não foi possível encontrar os elementos das coleções corretamente.')\n",
        "        else:\n",
        "          # Percorrendo todos os elementos de coleção\n",
        "          for indice, colecao in enumerate(colecoes):\n",
        "            nome_e_link_colecao = colecao.find('a', href=re.compile('/handle')) # Coletando o nome da coleção\n",
        "            try:\n",
        "              nome_colecao = nome_e_link_colecao.text\n",
        "              dic['Colecoes'].append(nome_colecao)\n",
        "\n",
        "              link_colecao = nome_e_link_colecao['href']\n",
        "              link_colecao = 'https://repositorio.ufsc.br'+link_colecao\n",
        "              dic['Links colecoes'].append(link_colecao)\n",
        "            except:\n",
        "              print(f'\\nErro ao encontrar o nome e link da coleção número {indice} da lista.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "              lista_de_falhas['Avisos'].append(f'Erro ao encontrar o nome e link da coleção número {indice} da lista.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "            # break # Só vai pegar a primeira coleção\n",
        "            limite_colecoes = 5\n",
        "            if indice >= limite_colecoes:\n",
        "              print(f'\\nLIMITE DE COLEÇÕES ({limite_colecoes}) ATINGIDO.')\n",
        "              break\n",
        "\n",
        "          print('\\nNúmero de coleções encontradas:',len(dic['Links colecoes']))\n",
        "          num_colecao = 0\n",
        "          # Percorrendo a listagem de links das coleções\n",
        "          for i in range(len(dic['Links colecoes'])):\n",
        "            dic_trabalho = {'Links trabalhos':[]}\n",
        "            num_colecao +=1\n",
        "\n",
        "            # Enviaremos uma requisição para a página de busca em cada coleção, na primeira página de resultados com um\n",
        "            # total de 100 resultados por página (padrão é 10, mas podemos alterar isso mexendo no link da requisição.\n",
        "            # Escolhido foi 100 que é um número 10 vezes maior que o padrão e a resposta vem em, aproximadamente, 10s)\n",
        "            link_colecao_atual = dic['Links colecoes'][i] + '/discover?rpp=5&etal=0&group_by=none&page=1'\n",
        "\n",
        "        #                                       MUDAR O RPP = 5 PARA RPP = 100 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "            try:\n",
        "              site = requests.get(link_colecao_atual,headers=headers,timeout=timeout)\n",
        "            except:\n",
        "              print(f'Ocorreu um erro durante a requisição com link \"{link_colecao_atual}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "              lista_de_falhas['Erros'].append(f'Ocorreu um erro durante a requisição com link \"{link_colecao_atual}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "              break\n",
        "            else:\n",
        "              res_code = site.status_code\n",
        "              if res_code == 200:\n",
        "                soup = BeautifulSoup(site.content, 'html.parser')\n",
        "                # Outro modo de encontrar objetos dentro do conteúdo html (agora o número de páginas)\n",
        "                numero_paginas = soup.find('div', attrs={\"class\":'pagination-masked top'})\n",
        "                if numero_paginas == None:\n",
        "                  print('\\nNão foi possível achar número de páginas corretamente.\\n(numero_paginas == None)')\n",
        "                  lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (numero_paginas == None).')\n",
        "                else:\n",
        "                  paginas_links = numero_paginas.find('ul',class_='pagination-links')\n",
        "                  if paginas_links == None:\n",
        "                    print('\\nNão foi possível achar número de páginas corretamente.\\n(paginas_links == None)')\n",
        "                    lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (paginas_links == None).')\n",
        "                  else:\n",
        "                    links_pagina = paginas_links.find_all('li')\n",
        "                    if links_pagina == None:\n",
        "                      print('\\nNão foi possível achar número de páginas corretamente.\\n(links_pagina == None)')\n",
        "                      lista_de_falhas[\"Avisos\"].append(f'Não foi possível achar número de páginas corretamente da coleção {dic[\"Colecoes\"][i]} --> (links_pagina == None).')\n",
        "                    else:\n",
        "                      try:\n",
        "                        numero_paginas = int(links_pagina[len(links_pagina)-1].find('a').text)\n",
        "                      except:\n",
        "                        print('\\nNúmero de páginas não pode ser convertido para inteiro, logo foi setado como 1.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                        numero_paginas = 1\n",
        "                        lista_de_falhas[\"Avisos\"].append(f'Número de páginas da coleção {dic[\"Colecoes\"][i]} não pode ser convertido para inteiro, logo foi setado como 1\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      print('\\nNúmero total de páginas:',numero_paginas)\n",
        "\n",
        "                      # Encontrando todos os links de teses e dissertações dentro do campo do resultado de busca dos mesmos\n",
        "                      links = soup.find_all('li', class_=re.compile('ds-artifact-item clearfix'))\n",
        "                      if len(links) == 0:\n",
        "                        print('\\nNão foi possível encontrar corretamente os links para os trabalhos.')\n",
        "                        lista_de_falhas[\"Avisos\"].append(f'Não foi possível encontrar/captar corretamente os links para os trabalhos da coleção {dic[\"Colecoes\"][i]}.')\n",
        "                      else:\n",
        "                        pagina = 1\n",
        "                        # Entraremos num loop que percorrerá todas as páginas de resultado da parte de busca da coleção\n",
        "                        while(pagina <= numero_paginas):\n",
        "                          print('\\nIndo para página:',pagina)\n",
        "                          link = dic['Links colecoes'][i] + f'/discover?rpp=5&etal=0&group_by=none&page={pagina}'\n",
        "                #                                       MUDAR O RPP = 5 PARA RPP = 100 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                          try:\n",
        "                            site = requests.get(link,headers=headers,timeout=timeout)\n",
        "                          except:\n",
        "                            print(f'Ocorreu um erro durante a requisição com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                            lista_de_falhas['Erros'].append(f'Ocorreu um erro durante a requisição com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                            break\n",
        "                          else:\n",
        "                            res_code = site.status_code\n",
        "                            if res_code == 200:\n",
        "                              soup = BeautifulSoup(site.content, 'html.parser')\n",
        "                              links = soup.find_all('li', class_=re.compile('ds-artifact-item clearfix'))\n",
        "                              if len(links) == 0:\n",
        "                                print(f'\\nNão foi possível encontrar corretamente os links dos resultados da coleção {dic[\"Colecoes\"][i]} no link: \"{link}\".')\n",
        "                                lista_de_falhas['Avisos'].append(f'Não foi possível encontrar/coletar corretamente os links dos resultados da coleção {dic[\"Colecoes\"][i]} no link: \"{link}\".')\n",
        "                              else:\n",
        "                                for links_trabalho in links:\n",
        "                                  link_trabalho = links_trabalho.find('a', href=re.compile('/handle'))\n",
        "                                  if link_trabalho != None:\n",
        "                                    link_trabalho = link_trabalho['href']\n",
        "                                    link_trabalho = 'https://repositorio.ufsc.br'+link_trabalho\n",
        "                                    dic_trabalho['Links trabalhos'].append(link_trabalho)\n",
        "                                    numero_total_de_trabalhos += 1\n",
        "                                  else:\n",
        "                                    print(f'\\nLink do trabalho não foi encontrado corretamente. Coleção: {dic[\"Colecoes\"][i]}, link: \"{link}\".')\n",
        "                                    lista_de_falhas['Avisos'].append(f'Link do trabalho não foi encontrado corretamente. Coleção: {dic[\"Colecoes\"][i]}, link: \"{link}\".')\n",
        "                            else:\n",
        "                              print(f'\\nErro de conexão com o site (internet fora do ar ou servidor do repositório fora do ar). Coleção que parou: {dic[\"Colecoes\"][i]}.')\n",
        "                              lista_de_falhas['Erros'].append(f'Erro de conexão com o site do repositório \"{link}\". Possivelmente internet instável/fora do ar ou servidor do repositório fora do ar). Coleção que parou: {dic[\"Colecoes\"][i]}.')\n",
        "\n",
        "                            # Limitando a busca (primeiramente não vamos passsar por todos os resultados)\n",
        "                            limite_pagina = 1\n",
        "                            if pagina >= limite_pagina:\n",
        "                              print(f'\\nLIMITE DE PÁGINAS ({limite_pagina}) ALCANÇADO.')\n",
        "                              break\n",
        "                            else:\n",
        "                              pagina += 1\n",
        "\n",
        "                        print('\\nNúmero de links captados:',len(dic_trabalho[\"Links trabalhos\"]))\n",
        "\n",
        "                      df = pd.DataFrame(dic_trabalho)\n",
        "\n",
        "                      # Tratando possíveis erros na hora de salvar o arquivo\n",
        "                      if '/' in dic[\"Colecoes\"][i]:\n",
        "                        index = dic[\"Colecoes\"][i].find('/')\n",
        "                        dic[\"Colecoes\"][i] = dic[\"Colecoes\"][i][:index]+'_'+dic[\"Colecoes\"][i][index+1:]\n",
        "                      if ':' in dic[\"Colecoes\"][i]:\n",
        "                        index = dic[\"Colecoes\"][i].find(':')\n",
        "                        dic[\"Colecoes\"][i] = dic[\"Colecoes\"][i][:index]+' '+dic[\"Colecoes\"][i][index+1:]\n",
        "                      try:\n",
        "                        df.to_csv(f'/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/{dic[\"Colecoes\"][i]}_Links_dos_Trabalhos.csv',index=False,encoding='utf-8',sep=',')\n",
        "                      except:\n",
        "                        print(f'\\nErro ao fazer download do arquivo .csv para coleção {dic[\"Colecoes\"][i]}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                        lista_de_falhas['Erros'].append(f'Erro ao fazer download do arquivo .csv para coleção {dic[\"Colecoes\"][i]}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      else:\n",
        "                        print(f'\\nDownload do arquivo .csv da coleção {dic[\"Colecoes\"][i]} feito com sucesso!\\n')\n",
        "                      # Código abaixo comentado, pois era apenas para fins de limitar o número de coleções que serão visitadas\n",
        "                      # num_colecao_maxima = 4\n",
        "                      # if num_colecao >= num_colecao_maxima:\n",
        "                      #   print(f'Parou na {num_colecao} coleção.')\n",
        "                      #   break\n",
        "          print('='*100)\n",
        "          print('Programa executado com sucesso!\\nNúmero total de links de trabalhos coletados:',numero_total_de_trabalhos)\n",
        "          print(f'Número de Coleções visitadas:',len(dic[\"Links colecoes\"]))\n",
        "          print('='*100)\n",
        "\n",
        "      else: # Caso a localização ou identificação do campo de coleções tenha mudado, ocorrerá um erro e seremos notificados.\n",
        "        print('\\nCampo das coleções não identificado corretamente.')\n",
        "        lista_de_falhas['Erros'].append(\"Campo das coleções 'find('div', class_='ds-static-div secondary')' não identificado corretamente.\")\n",
        "\n",
        "    else: # Caso a internet esteja ruim ou o servidor da UFSC esteja fora do ar, ocorrerá um erro e seremos notificados.\n",
        "      print('\\nErro de conexão com o site (internet fora do ar ou servidor do repositório fora do ar)...')\n",
        "      lista_de_falhas['Erros'].append(f'Erro de conexão com o site do repositório \"{link}\". Possivelmente internet instável/fora do ar ou servidor do repositório fora do ar.')\n",
        "\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Número de avisos gerados:',len(lista_de_falhas[\"Avisos\"]))\n",
        "  print('Número de erros gerados:',len(lista_de_falhas[\"Erros\"]))\n",
        "  print('='*100)\n",
        "  # Salvando arquivos de Avisos e Erros ao decorrer desta execução\n",
        "  if (len(lista_de_falhas['Avisos']) > 0):\n",
        "    file_path = '/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/1 AVISOS.txt'\n",
        "    texto = 'AVISOS:\\n\\n'\n",
        "    for i in range(len(lista_de_falhas['Avisos'])):\n",
        "      texto = texto + '\\n' + lista_de_falhas['Avisos'][i]\n",
        "    with open(file_path, 'w') as f:\n",
        "      f.write(texto)\n",
        "      print('\\nArquivo de avisos salvo com sucesso.\\nVerifique-o!')\n",
        "      f.close()\n",
        "  if (len(lista_de_falhas['Erros']) > 0):\n",
        "    file_path = '/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/1 ERROS.txt'\n",
        "    texto = 'ERROS:\\n\\n'\n",
        "    for i in range(len(lista_de_falhas['Erros'])):\n",
        "      texto = texto + '\\n' + lista_de_falhas['Erros'][i]\n",
        "    with open(file_path, 'w') as f:\n",
        "      f.write(texto)\n",
        "      print('\\nArquivo de erros salvo com sucesso.\\nVerifique-o!')\n",
        "      f.close()\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  duracao = end - start\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Duração total da primeira execução:',round(duracao,2))\n",
        "  print('='*100)\n",
        "else:\n",
        "  print('\\nFalha na execução das células anteriores. Volte, leia as saídas das células anteriores e tente executá-las novamente.\\n')"
      ],
      "metadata": {
        "id": "FkPZFaUuhGb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 6** - Entra link por link em cada coleção e coleta os metadados, entra no link do PDF, baixa o PDF, extrai texto do PDF, exclui arquivo PDF, criar arquivo .txt com o texto extraído e salva arquivo .txt."
      ],
      "metadata": {
        "id": "tr_k2UmJhKHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if execucao:\n",
        "  start2 = time.time()\n",
        "  try:\n",
        "    arquivos_pasta_links = os.listdir('/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC')\n",
        "    if not arquivos_pasta_links:\n",
        "      print(f'\\nA pasta \"Links dos trabalhos - Repositorio UFSC\" está vazia.\\n')\n",
        "      execucao = False\n",
        "    else:\n",
        "      print('\\nArquivos de links foram encontrados na pasta \"Links dos trabalhos - Repositorio UFSC\".')\n",
        "  except:\n",
        "    print(f'\\nErro ao carregar arquivos da pasta \"Links dos trabalhos - Repositorio UFSC\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "    execucao = False\n",
        "\n",
        "\n",
        "  if execucao:\n",
        "    for numeracao, arquivo in enumerate(arquivos_pasta_links):\n",
        "      if (arquivo == '1 AVISOS.txt') or (arquivo == '1 ERROS.txt'):\n",
        "        pass\n",
        "      else:\n",
        "        lista_de_falhas_trabalho = {'Erros':[],'Avisos':[]}\n",
        "        try:\n",
        "          df = pd.read_csv(f'/content/drive/MyDrive/Coleções Repositorio UFSC/Links dos trabalhos - Repositorio UFSC/{arquivo}',encoding='utf-8', sep=',')\n",
        "        except:\n",
        "          print('\\nErro ao abrir arquivo dos Links dos trabalhos.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "          lista_de_falhas_trabalho['Erros'].append('Erro ao abrir arquivo dos Links dos trabalhos.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "        else:\n",
        "          lista_links_trabalhos = df[\"Links trabalhos\"]\n",
        "          dic = {'Título':[],'Autor':[],'Resumo':[],'Descrição':[],'Data':[],'Links':[],'Link do PDF':[]}\n",
        "\n",
        "          for contagem, link in enumerate(lista_links_trabalhos):\n",
        "            site = requests.get(link,headers=headers)\n",
        "            res_code = site.status_code\n",
        "            if res_code == 200:\n",
        "              soup = BeautifulSoup(site.content, 'html.parser')\n",
        "              tabela_info = soup.find('table', class_='ds-includeSet-table')\n",
        "              tabela_arquivos = soup.find('table', attrs={\"class\":'ds-table file-list'})\n",
        "\n",
        "              if (tabela_info == None) or (tabela_arquivos == None):\n",
        "                print('\\nNão foi possível encontrar corretamente as informações gerais do trabalho.')\n",
        "                lista_de_falhas_trabalho['Avisos'].append(f'Não foi possível encontrar corretamente as informações gerais do trabalho com link: \"{link}\".\\ntabela_info == None or tabela_arquivo == None.')\n",
        "                dic[\"Data\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Descrição\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Link do PDF\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Resumo\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Texto do PDF\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Título\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Autor\"].append('ERRO AO ENCONTRAR')\n",
        "                dic[\"Links\"].append(link)\n",
        "              else:\n",
        "                print('\\nPegando informações gerais...')\n",
        "                nao_eh_TCC = True\n",
        "                titulo = autor = resumo = descricao = url = data = 'NÃO LOCALIZADO'\n",
        "                for info in tabela_info:\n",
        "                  analise = info.text\n",
        "                  if 'Título' in analise[:(len('Título')+2)]:\n",
        "                    titulo = analise\n",
        "                    titulo = titulo[(len('titulo:')+2):].strip()\n",
        "                  elif 'Autor' in analise[:len('Autor')+2]:\n",
        "                    autor = analise\n",
        "                    autor = autor[(len('autor:')+2):].strip()\n",
        "                  elif 'Resumo' in analise[:len('Resumo')+2]:\n",
        "                    resumo = analise\n",
        "                    resumo = resumo[(len('resumo:')+2):].strip()\n",
        "                  elif 'Descrição' in analise[:len('Descrição')+2]:\n",
        "                    descricao = analise\n",
        "                    descricao = descricao[(len('descricao:')+2):].strip()\n",
        "                    if ('TCC' not in descricao[:len('TCC')+2]) and ('Trabalho de Conclusão de Curso' not in descricao[:len('Trabalho de Conclusão de Curso')+2]):\n",
        "                      nao_eh_TCC = True\n",
        "                    else:\n",
        "                      nao_eh_TCC = False\n",
        "                  elif ('URI' in analise[:len('URI')+2]) or ('URL' in analise[:len('URL')+2]):\n",
        "                    url = analise\n",
        "                    url = url[(len('url:')+2):].strip()\n",
        "                  elif 'Data' in analise[:len('Data')+2]:\n",
        "                    data = analise\n",
        "                    data = data[(len('data:')+2):].strip()\n",
        "                  else:\n",
        "                    pass\n",
        "                if nao_eh_TCC == True:\n",
        "                  print('Pegando informações dos arquivos...')\n",
        "                  for info_arq in tabela_arquivos:\n",
        "                    analise = info_arq.text\n",
        "                    if ('PDF' in analise) and ('.pdf' in analise):\n",
        "                      print('\\t--> Opa, esse aqui tem PDF...')\n",
        "                      link_pdf = info_arq.find('a')\n",
        "                      if 'href=\"' in str(link_pdf):\n",
        "                        link_pdf = link_pdf['href']\n",
        "                        link_pdf = 'https://repositorio.ufsc.br'+link_pdf\n",
        "\n",
        "                        # Código para extração de texto de PDF (pdfplumber)\n",
        "                        try:\n",
        "                          index = arquivo.find('_Links_dos_Trabalhos.csv')\n",
        "                          nome_arquivo = arquivo[:index]\n",
        "                          local_PDF = False\n",
        "                          if (CriaDiretorio(f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}')):\n",
        "                            local_PDF = True\n",
        "                          else:\n",
        "                            print(f'\\nErro na hora de chamar a função para criação da pasta \"/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}\".')\n",
        "                            lista_de_falhas_trabalho['Avisos'].append(f'\\nErro na hora de chamar a função para criação da pasta \"/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}\".')\n",
        "                        except:\n",
        "                          print(f'\\nErro na criação da pasta \"/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                          lista_de_falhas_trabalho['Avisos'].append(f'Erro na criação da pasta \"/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                        else:\n",
        "                          if local_PDF:\n",
        "                            try:\n",
        "                              caminho_arquivo = f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}/arquivoPDF_temporario.pdf'\n",
        "                              request.urlretrieve(link_pdf, caminho_arquivo)\n",
        "                            except:\n",
        "                              print(f'\\nProblema ao baixar o PDF de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                              lista_de_falhas_trabalho['Avisos'].append(f'Problema ao baixar o PDF de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                            else:\n",
        "                              print(f'\\nPDF temporário do trabalho {contagem} baixado com sucesso.')\n",
        "\n",
        "                              try:\n",
        "                                with pdfplumber.open(caminho_arquivo) as pdf:\n",
        "                                  diferencas = []\n",
        "                                  i = 1\n",
        "                                  j = 1\n",
        "                                  t = 0\n",
        "                                  for pdf_page in pdf.pages:\n",
        "                                    single_page_text = pdf_page.extract_text()\n",
        "                                    try:\n",
        "                                      primeiras_letras = re.findall(r'[0-9][0-9]|[0-9]',single_page_text[0:2].strip())\n",
        "                                      if len(primeiras_letras) > 0:\n",
        "                                        pagina_numerada = int(primeiras_letras[0])\n",
        "                                        diferencas.append(pdf_page.page_number - pagina_numerada)\n",
        "                                        i += 1\n",
        "                                      ultimas_letras = re.findall(r'[0-9][0-9]|[0-9]',single_page_text[(len(single_page_text)-1)-2:].strip())\n",
        "                                      if len(ultimas_letras) > 0:\n",
        "                                        pagina_numerada = int(ultimas_letras[0])\n",
        "                                        diferencas.append(pdf_page.page_number - pagina_numerada)\n",
        "                                        j += 1\n",
        "                                    except:\n",
        "                                      pass\n",
        "                                    else:\n",
        "                                      t += 1\n",
        "                                      single_page_text = single_page_text.strip()\n",
        "\n",
        "                                    if i>=3:\n",
        "                                      if (diferencas[len(diferencas)-1] == diferencas[len(diferencas)-2]) and (diferencas[len(diferencas)-2] == diferencas[len(diferencas)-3]):\n",
        "                                        break\n",
        "                                      else:\n",
        "                                        i = 0\n",
        "                                    elif j >=3:\n",
        "                                      if (diferencas[len(diferencas)-1] == diferencas[len(diferencas)-2]) and (diferencas[len(diferencas)-2] == diferencas[len(diferencas)-3]):\n",
        "                                        break\n",
        "                                      else:\n",
        "                                        j = 0\n",
        "                                    elif t >= 20:\n",
        "                                      diferencas.append(0)\n",
        "                                      break\n",
        "\n",
        "                                  diferenca = diferencas[len(diferencas)-1]\n",
        "\n",
        "                                  all_text = ''\n",
        "                                  for pdf_page in pdf.pages:\n",
        "                                    single_page_text = pdf_page.extract_text()\n",
        "                                    try:\n",
        "                                      if single_page_text.startswith(str(pdf_page.page_number-diferenca)):\n",
        "                                        # print('PAGINA COMEÇA COM NÚMERO DE PAGINAS')\n",
        "                                        single_page_text = single_page_text[len(str(pdf_page.page_number-diferenca))+1:]\n",
        "                                      elif single_page_text.endswith(str(pdf_page.page_number-diferenca)):\n",
        "                                        # print('PAGINA TERMINA COM NÚMERO DE PAGINAS')\n",
        "                                        single_page_text = single_page_text[:(len(single_page_text)-1)-2]\n",
        "                                    except:\n",
        "                                      pass\n",
        "                                    else:\n",
        "                                      single_page_text = single_page_text.replace('\\n',' ').strip()\n",
        "\n",
        "                                    all_text = all_text + '\\n' + single_page_text\n",
        "\n",
        "                                  # print(all_text)\n",
        "                                  pdf.close()\n",
        "                                  # tokens = list(all_text.lower().split())\n",
        "                                  # print('\\nTamanho da lista de tokens:',len(tokens))\n",
        "                                try:\n",
        "                                  os.remove(caminho_arquivo)\n",
        "                                except:\n",
        "                                  print(f'Problema ao excluir PDF temporário de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                                  lista_de_falhas_trabalho['Avisos'].append(f'Problema ao excluir PDF temporário de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                                else:\n",
        "                                  print('PDF temporário excluído com sucesso.')\n",
        "                                try:\n",
        "                                  with open(f\"/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}/PDF_texto.txt\", \"w\") as arq:\n",
        "                                    arq.write(all_text)\n",
        "                                    arq.close()\n",
        "                                except:\n",
        "                                  print(f'Problema ao salvar o arquivo como .txt. do conteúdo do PDF temporário de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                                  lista_de_falhas_trabalho['Avisos'].append(f'Problema ao salvar o arquivo como .txt. do conteúdo do PDF temporário de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                                else:\n",
        "                                  print('Arquivo .txt com o texto do PDF criado com sucesso!')\n",
        "                              except:\n",
        "                                print(f'Problema ao abrir/ler o conteúdo do PDF de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\" e/ou salvar o arquivo como .txt.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "                                lista_de_falhas_trabalho['Avisos'].append(f'Problema ao abrir/ler o conteúdo do PDF de {nome_arquivo}, trabalho {contagem}, com link \"{link_pdf}\" e/ou salvar o arquivo como .txt.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                              for arquivo_temp in os.listdir(f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}'):\n",
        "                                if arquivo_temp.lower().endswith('.pdf'):\n",
        "                                  caminho_do_arquivo = os.path.join(f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}', arquivo_temp)\n",
        "                                  os.remove(caminho_do_arquivo)\n",
        "                                  print(f'Arquivo {arquivo_temp} removido forçadamente, após problema.')\n",
        "                      try:\n",
        "                        dic[\"Data\"].append(data)\n",
        "                      except:\n",
        "                        dic[\"Data\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar DATA do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Descrição\"].append(descricao)\n",
        "                      except:\n",
        "                        dic[\"Descrição\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar DESCRIÇÃO do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Link do PDF\"].append(link_pdf)\n",
        "                      except:\n",
        "                        dic[\"Link do PDF\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar LINK DO PDF do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Resumo\"].append(resumo)\n",
        "                      except:\n",
        "                        dic[\"Resumo\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar RESUMO do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Título\"].append(titulo)\n",
        "                      except:\n",
        "                        dic[\"Título\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar TÍTULO do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Autor\"].append(autor)\n",
        "                      except:\n",
        "                        dic[\"Autor\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar AUTOR do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      try:\n",
        "                        dic[\"Links\"].append(link)\n",
        "                      except:\n",
        "                        dic[\"Links\"].append('ERRO AO ENCONTRAR')\n",
        "                        lista_de_falhas_trabalho[\"Avisos\"].append(f'Problema ao encontrar LINK do trabalho com link \"{link}\".\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "                      # Estaremos pegando apenas o primeiro PDF !!!\n",
        "                      # (alguns trabalhos tem mais de um PDF publicado,\n",
        "                      # temos que analisar esse cenário com mais cautela ainda). Esse break é usado para isso..\n",
        "                      break\n",
        "\n",
        "                else:\n",
        "                  print('\\nEncontramos um TCC!\\n\\t--> Descartando arquivos...\\n')\n",
        "\n",
        "            else:\n",
        "              print('\\nErro de conexão com o site. Possivelmente internet instável/fora do ar ou servidor do repositório fora do ar.')\n",
        "              lista_de_falhas_trabalho[\"Erros\"].append(f'Erro de conexão com o site do repositório \"{link}\". Possivelmente internet instável/fora do ar ou servidor do repositório fora do ar.')\n",
        "\n",
        "            if (len(lista_de_falhas_trabalho['Avisos']) > 0):\n",
        "              file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}/1 AVISOS.txt'\n",
        "              texto = 'AVISOS:\\n\\n'\n",
        "              for i in range(len(lista_de_falhas_trabalho['Avisos'])):\n",
        "                texto = texto + '\\n' + lista_de_falhas_trabalho['Avisos'][i]\n",
        "              with open(file_path, 'w') as f:\n",
        "                f.write(texto)\n",
        "                print(f'\\nArquivo de avisos salvo com sucesso para coleção {nome_arquivo} e trabalho {contagem}.\\nVerifique-o!')\n",
        "                f.close()\n",
        "            if (len(lista_de_falhas_trabalho['Erros']) > 0):\n",
        "              file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Trabalho {contagem}/1 ERROS.txt'\n",
        "              texto = 'ERROS:\\n\\n'\n",
        "              for i in range(len(lista_de_falhas_trabalho['Erros'])):\n",
        "                texto = texto + '\\n' + lista_de_falhas_trabalho['Erros'][i]\n",
        "              with open(file_path, 'w') as f:\n",
        "                f.write(texto)\n",
        "                print(f'\\nArquivo de erros salvo com sucesso para coleção {nome_arquivo} e trabalho {contagem}.\\nVerifique-o!')\n",
        "                f.close()\n",
        "\n",
        "            # break # Só passará pelo primeiro link dos links de trabalhos\n",
        "            if contagem >= 4: # Só passará pelos 5 primeiros links do arquivo que tem os links de trabalhos\n",
        "              break\n",
        "\n",
        "            contagem += 1\n",
        "\n",
        "          # Criando avisos gerais na pasta de cada Coleção\n",
        "          diretorio_raiz = f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}'\n",
        "          pastas_com_arquivo_avisos = {}\n",
        "          for diretorio_atual, subdiretorios, arquivos in os.walk(diretorio_raiz):\n",
        "            if '1 AVISOS.txt' in arquivos:\n",
        "              pasta_pai = os.path.basename(os.path.dirname(diretorio_atual))\n",
        "              pasta_com_arquivo = os.path.basename(diretorio_atual)\n",
        "              try:\n",
        "                pastas_com_arquivo_avisos[str(pasta_pai)].append(pasta_com_arquivo)\n",
        "              except:\n",
        "                pastas_com_arquivo_avisos[str(pasta_pai)] = [pasta_com_arquivo]\n",
        "          # print(\"Pastas que contém o arquivo '1 AVISOS.txt':\")\n",
        "          # print(pastas_com_arquivo_avisos)\n",
        "          if (len(pastas_com_arquivo_avisos)> 0):\n",
        "            file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/1 AVISOS.txt'\n",
        "            texto = 'AVISOS encontrados nos trabalhos:\\n\\n'\n",
        "            for pasta in pastas_com_arquivo_avisos:\n",
        "              for i in range(len(pastas_com_arquivo_avisos[pasta])):\n",
        "                texto = texto + pastas_com_arquivo_avisos[pasta][i] + '\\n'\n",
        "\n",
        "            with open(file_path, 'w') as f:\n",
        "              f.write(texto)\n",
        "              print(f'\\nArquivo de avisos gerais da coleção \"{nome_arquivo}\" salvo com sucesso.\\nVerifique-o!')\n",
        "              f.close()\n",
        "\n",
        "          try:\n",
        "            df = pd.DataFrame(dic)\n",
        "            df.to_csv(f'/content/drive/MyDrive/Coleções Repositorio UFSC/{nome_arquivo}/Meta_dados_{numeracao}.csv', index=False, encoding='utf-8', sep=',')\n",
        "          except:\n",
        "            print(f'\\nErro ao fazer download do arquivo .csv para coleção {nome_arquivo}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\":\",str(sys.exc_info()[1])+'\\n')\n",
        "            lista_de_falhas_trabalho[\"Erros\"].append(f'Erro ao fazer download do arquivo .csv para coleção {nome_arquivo}.\\nDescrição do erro --> '+str(sys.exc_info()[0].__name__)+\": \"+str(sys.exc_info()[1])+'\\n')\n",
        "          else:\n",
        "            print(f'\\nDownload do arquivo .csv da coleção {nome_arquivo} feito com sucesso!')\n",
        "\n",
        "\n",
        "\n",
        "  else:\n",
        "    print('\\nFalha ao abrir e carregar os arquivos presentes na pasta das coleções.\\nTente novamente em alguns minutos.\\n')\n",
        "\n",
        "\n",
        "  end2 = time.time()\n",
        "  duracao2 = end2 - start2\n",
        "  print('\\n')\n",
        "  print('='*100)\n",
        "  print('Duração total da segunda execução:',round(duracao2,2))\n",
        "  print('='*100)\n",
        "\n",
        "else:\n",
        "  print('\\nFalha no carregamento das bibliotecas e/ou na sincronização e criação de pasta do seu Google Drive. Tente executar a primeira e segunda células de códigos novamente.\\n')"
      ],
      "metadata": {
        "id": "2Fj6jeBhhM7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execução 7** - Final: Passa por todas as pastas criadas a fim de analisar se existem arquivos de avisos e erros e os informa no início das pastas \"raízes\""
      ],
      "metadata": {
        "id": "NIo5sf4VhQss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diretorio_raiz = '/content/drive/MyDrive/Coleções Repositorio UFSC'\n",
        "\n",
        "pastas_com_arquivo_avisos = {}\n",
        "\n",
        "for diretorio_atual, subdiretorios, arquivos in os.walk(diretorio_raiz):\n",
        "  if '1 AVISOS.txt' in arquivos:\n",
        "    pasta_pai = os.path.basename(os.path.dirname(diretorio_atual))\n",
        "    pasta_com_arquivo = os.path.basename(diretorio_atual)\n",
        "    try:\n",
        "      pastas_com_arquivo_avisos[str(pasta_pai)].append(pasta_com_arquivo)\n",
        "    except:\n",
        "      pastas_com_arquivo_avisos[str(pasta_pai)] = [pasta_com_arquivo]\n",
        "\n",
        "print(\"Pastas que contém o arquivo '1 AVISOS.txt':\")\n",
        "print(pastas_com_arquivo_avisos)\n",
        "\n",
        "if (len(pastas_com_arquivo_avisos)> 0):\n",
        "  file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/1 AVISOS.txt'\n",
        "  texto = 'AVISOS encontrados nas pastas:\\n\\n'\n",
        "  for pasta in pastas_com_arquivo_avisos:\n",
        "    texto = texto + pasta + '\\n\\n'\n",
        "    # for i in range(len(pastas_com_arquivo_avisos[pasta])):\n",
        "    #   texto = texto + '\\n' + pastas_com_arquivo_avisos[pasta][i]\n",
        "    # texto = texto + '\\n\\n'\n",
        "  with open(file_path, 'w') as f:\n",
        "    f.write(texto)\n",
        "    print(f'\\nArquivo de avisos gerais salvo com sucesso.\\nVerifique-o!')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "pastas_com_arquivo_erros = {}\n",
        "\n",
        "for diretorio_atual, subdiretorios, arquivos in os.walk(diretorio_raiz):\n",
        "  if '1 ERROS.txt' in arquivos:\n",
        "    pasta_pai = os.path.basename(os.path.dirname(diretorio_atual))\n",
        "    pasta_com_arquivo = os.path.basename(diretorio_atual)\n",
        "    try:\n",
        "      pastas_com_arquivo_erros[str(pasta_pai)].append(pasta_com_arquivo)\n",
        "    except:\n",
        "      pastas_com_arquivo_erros[str(pasta_pai)] = [pasta_com_arquivo]\n",
        "\n",
        "print(\"\\nPastas que contém o arquivo '1 ERROS.txt':\")\n",
        "print(pastas_com_arquivo_erros)\n",
        "\n",
        "if (len(pastas_com_arquivo_erros)> 0):\n",
        "  file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/1 ERROS.txt'\n",
        "  texto = 'ERROS encontrados nas pastas:\\n\\n'\n",
        "  for pasta in pastas_com_arquivo_erros:\n",
        "    texto = texto + pasta + '\\n\\n'\n",
        "    # for i in range(len(pastas_com_arquivo_avisos[pasta])):\n",
        "    #   texto = texto + '\\n' + pastas_com_arquivo_avisos[pasta][i]\n",
        "    # texto = texto + '\\n\\n'\n",
        "  with open(file_path, 'w') as f:\n",
        "    f.write(texto)\n",
        "    print(f'\\nArquivo de erros gerais salvo com sucesso.\\nVerifique-o!')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "pastas_com_arquivo_pdfs = {}\n",
        "\n",
        "for diretorio_atual, subdiretorios, arquivos in os.walk(diretorio_raiz):\n",
        "  for arquivo_pdf in arquivos:\n",
        "    if arquivo_pdf.lower().endswith('.pdf'):\n",
        "      pasta_pai = os.path.basename(os.path.dirname(diretorio_atual))\n",
        "      pasta_com_arquivo = os.path.basename(diretorio_atual)\n",
        "      try:\n",
        "        pastas_com_arquivo_pdfs[str(pasta_pai)].append(pasta_com_arquivo)\n",
        "      except:\n",
        "        pastas_com_arquivo_pdfs[str(pasta_pai)] = [pasta_com_arquivo]\n",
        "      # caminho_arquivo = os.path.join(diretorio_atual, arquivo_pdf)\n",
        "      # os.remove(caminho_arquivo)\n",
        "\n",
        "print(\"\\nPastas que contém arquivo .pdf:\")\n",
        "print(pastas_com_arquivo_pdfs)\n",
        "\n",
        "if (len(pastas_com_arquivo_pdfs)> 0):\n",
        "  file_path = f'/content/drive/MyDrive/Coleções Repositorio UFSC/1 PDFs restantes.txt'\n",
        "  texto = 'PDFs encontrados nas pastas:\\n\\n'\n",
        "  for pasta in pastas_com_arquivo_pdfs:\n",
        "    texto = texto + f'Pasta --> {pasta}'\n",
        "    for i in range(len(pastas_com_arquivo_avisos[pasta])):\n",
        "      texto = texto + '\\n' + pastas_com_arquivo_avisos[pasta][i]\n",
        "    texto = texto + '\\n\\n'\n",
        "  with open(file_path, 'w') as f:\n",
        "    f.write(texto)\n",
        "    print(f'\\nArquivo de erros gerais salvo com sucesso.\\nVerifique-o!')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "Q628H8C_hRVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##testando commits"
      ],
      "metadata": {
        "id": "D_kFZTrX83K1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}